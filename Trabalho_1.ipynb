{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHO6AESuxnGM"
      },
      "source": [
        "# Trabalho 1 - Nivelamento\n",
        "\n",
        "Considere os datasets abaixo, estratégias de pré-processamento, medidas de avaliação, métodos de comparação estatística e os seguintes algoritmos de aprendizado de máquina: árvore de decisão, random forest e k-nearest neighbor. A partir disso, responda as seguintes perguntas:\n",
        "\n",
        "1. Qual o algoritmo de AM mais adequado para cada problema?\n",
        "2. Qual o algoritmo de AM mais adequado para todos os problemas?\n",
        "knn:85.94066985255232\n",
        "Arvore de decisão:85.42638695604664\n",
        "Random Florest:86.62282691074151\n",
        "Partindo de uma comparação direta entre os resultados do cross validation usando o msm conjunto de dados para todos, o algoritmo de random florest é o melhor, no entanto é o mais demorado também demorando 31 minutos para processar tudo, enquanto so demais demoraram apenas 1 minuto cada, em segundo fica o knn e terceiro a arvore de decisão que tem tempos similares. As únicas alterações feitas no dataset foi a seleção de atributos, e a conversão de atributos nominais para numéricos/binários utilizando o getdummies ou a conversão direta para os que tinham apenas duas classes.\n",
        "\n",
        "Para responder essas questões construa um notebook no colab ou um ambiente similar. Documente de forma clara cada passo e justifique suas decisões."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "LOdk7BJHxmH4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "import sklearn.metrics as metrics\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_selection import RFE\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib as plt\n",
        "from scipy.io import arff\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "'''Link com os datasets'''\n",
        "PhishingWebsites_link = \"https://www.openml.org/data/download/1798106/phpV5QYya\"\n",
        "arrhythmia_link       = \"https://www.openml.org/data/download/53551/arrhythmia.arff\"\n",
        "Satellite_link        = \"https://www.openml.org/data/download/16787463/phpZrCzJR\"\n",
        "airlines_link         = \"https://www.openml.org/data/download/66526/phpvcoG8S\"\n",
        "AedesSex_link         = \"https://github.com/denismr/Classification-and-Counting-with-Recurrent-Contexts/raw/master/codeAndData/data/AedesSex.csv\"\n",
        "phoneme_link          = \"https://www.openml.org/data/download/1592281/php8Mz7BG\"\n",
        "adult_link            = \"https://www.openml.org/data/download/1595261/phpMawTba\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadDataset_Arff(url: str) -> pd.DataFrame:\n",
        "  '''Função responsável por ler os datasets do tipo .arff'''\n",
        "  ftpstream = urllib.request.urlopen(url)\n",
        "  return pd.DataFrame(arff.loadarff(io.StringIO(ftpstream.read().decode('utf-8')))[0])\n",
        "  \n",
        "def get_dummies(df: pd.DataFrame, col:str) -> pd.DataFrame:\n",
        "  '''Função responsável por separar os dados nominais em outras colunas.'''\n",
        "  return pd.get_dummies(df,prefix=col,prefix_sep='.',columns=[col]).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def proportion(Dataset,target=\"Target\"):\n",
        "  '''Função que gera um gráfico de proporção dos valores targets'''\n",
        "  target_count = Dataset[target].value_counts()\n",
        "  print('Class 0:', target_count[0])\n",
        "  print('Class 1:', target_count[1])\n",
        "  target_count.plot(kind='bar', title='Count (target)');\n",
        "  \n",
        "def normalize(dataset,target='Target') -> pd.DataFrame:\n",
        "  '''Função que retorna o dataset normalizado'''\n",
        "  X = dataset.drop([target], axis=1).copy()\n",
        "  Y = dataset[target].copy()\n",
        "\n",
        "  scaler = MinMaxScaler()\n",
        "  X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "  dataset = X\n",
        "  dataset[target] = Y\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "def knn(dataset,n_neighbors=3,target='Target'):\n",
        "  '''Função que aplica o knn'''\n",
        "  X_train, X_test, y_train, y_test = train_test_split(dataset.drop([target], axis=1), dataset[target], test_size=0.3)\n",
        "  model = KNeighborsClassifier(n_neighbors=3)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  print(accuracy_score(y_test, y_pred)*100)\n",
        "\n",
        "def treeDecision(dataset,target='Target'):\n",
        "  '''Função que aplica a arvore de decisão'''\n",
        "  X_train, X_test, y_train, y_test = train_test_split(dataset.drop([target], axis=1), dataset[target], test_size=0.3)\n",
        "  model = DecisionTreeClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  print(accuracy_score(y_test, y_pred)*100)\n",
        "\n",
        "def RForestDecision(dataset,target='Target'):\n",
        "  '''Função que aplica random florest'''\n",
        "  X_train, X_test, y_train, y_test = train_test_split(dataset.drop([target], axis=1), dataset[target], test_size=0.3)\n",
        "  model = RandomForestClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  print(accuracy_score(y_test, y_pred)*100)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def crossVal(x_Test ,y_Test,x_train,y_train):\n",
        "  '''Função que aplica o knn com x e y previamente setados'''\n",
        "  modelDT = DecisionTreeClassifier()\n",
        "  modelDT.fit(x_train,y_train)\n",
        "  modeRF = RandomForestClassifier()\n",
        "  modeRF.fit(x_train,y_train)\n",
        "  modelKNN = KNeighborsClassifier(n_neighbors=3)\n",
        "  modelKNN.fit(x_train,y_train)\n",
        "\n",
        "  return cross_val_score(modelDT, x_Test, y_Test, cv=10, scoring=\"f1\"),cross_val_score(modeRF, x_Test, y_Test, cv=10, scoring=\"f1\"),cross_val_score(modelKNN, x_Test, y_Test, cv=10, scoring=\"f1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "def selectAttributes(dataset):\n",
        "  '''Função que aplica o knn com x e y previamente setados'''\n",
        "  model = DecisionTreeClassifier()#max_leaf_nodes=10\n",
        "  feature_ = RFE(model,n_features_to_select=5, step=1)\n",
        "  return feature_.fit(dataset.drop(['Target'],axis=1),dataset['Target']).support_\n",
        "\n",
        "def removeColumns(dataset):\n",
        "  '''Função que faz a seleção de atributos e retorna uma lista com o nome das colunas que serão removidas'''\n",
        "  attributes = selectAttributes(dataset)\n",
        "  dropListIndex = np.where(attributes == False)[0]\n",
        "  dropList = []\n",
        "  for index in dropListIndex:\n",
        "    if(dataset.columns[index] != \"Target\"):\n",
        "      dropList.append(dataset.columns[index])\n",
        "  return dropList\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def underSampling(dataset):\n",
        "  # CONTAR CLASSES\n",
        "  count_class_1, count_class_0 = dataset.Target.value_counts()\n",
        "  # # Divide by class\n",
        "  df_class_0 = dataset[dataset['Target'] == 0]\n",
        "  df_class_1 = dataset[dataset['Target'] == 1]\n",
        "  df_class_1_under = df_class_1.sample(int(count_class_1*0.8))\n",
        "  df_test_under = pd.concat([df_class_1_under, df_class_0], axis=0)\n",
        "  return df_test_under\n",
        "\n",
        "def overSampling(dataset):\n",
        "  # CONTAR CLASSES\n",
        "  count_class_1, count_class_0 = dataset.Target.value_counts()\n",
        "  print(count_class_1, count_class_0)\n",
        "  # # Divide by class\n",
        "  df_class_0 = dataset[dataset['Target'] == 0]\n",
        "  df_class_1 = dataset[dataset['Target'] == 1]\n",
        "  df_class_0_over = df_class_0.sample(int(count_class_0/0.85),replace=True)\n",
        "  df_test_over = pd.concat([df_class_0_over, df_class_1], axis=0)\n",
        "  return df_test_over"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lendo todos os datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "InitDataset_PhishingWebsites = loadDataset_Arff(PhishingWebsites_link)\n",
        "InitDataset_arrhythmia = loadDataset_Arff(arrhythmia_link)\n",
        "InitDataset_Satellite = loadDataset_Arff(Satellite_link)\n",
        "InitDataset_airlines = loadDataset_Arff(airlines_link)\n",
        "InitDataset_AedesSex = pd.read_csv(AedesSex_link)\n",
        "InitDataset_phoneme = loadDataset_Arff(phoneme_link)\n",
        "InitDataset_adult = loadDataset_Arff(adult_link)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Abaixo será feito a conversão das classes targets para True e False, e trocado o nome da coluna para Target para as que não estão. Também será feito uma analise de distribuição e procura de dados nominais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "Dataset_PhishingWebsites = InitDataset_PhishingWebsites.copy()\n",
        "Dataset_PhishingWebsites['Target'] = [True if x == '-1'.encode() else False for x in Dataset_PhishingWebsites['Result']]\n",
        "Dataset_PhishingWebsites = Dataset_PhishingWebsites.drop(['Result'],axis=1)\n",
        "Dataset_PhishingWebsites = Dataset_PhishingWebsites.drop(removeColumns(Dataset_PhishingWebsites),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "Dataset_arrhythmia = InitDataset_arrhythmia.copy()\n",
        "Dataset_arrhythmia['Target'] = [True if x == 'P'.encode() else False for x in Dataset_arrhythmia['binaryClass']]\n",
        "Dataset_arrhythmia = Dataset_arrhythmia.drop(['binaryClass'],axis=1) #Remoção da coluna J por ter 98% dos dados NaN\n",
        "Dataset_arrhythmia = Dataset_arrhythmia.dropna() #Remoção das linhas com valores faltantes\n",
        "Dataset_arrhythmia = Dataset_arrhythmia.drop(removeColumns(Dataset_arrhythmia),axis=1) \n",
        "Dataset_arrhythmia.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "##Sem nominais usando a função select_nominals\n",
        "Dataset_Satellite = InitDataset_Satellite.copy()\n",
        "Dataset_Satellite['Target'] = [True if x == 'Anomaly'.encode() else False for x in Dataset_Satellite['Target']]\n",
        "Dataset_Satellite = Dataset_Satellite.drop(removeColumns(Dataset_Satellite),axis=1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "##Dataset com muitos dados nominais, para aplicar o knn será necessário usar a função getdummies em AirportFrom e AirportTo.\n",
        "Dataset_airlines = InitDataset_airlines.copy()\n",
        "Dataset_airlines['Target'] = [True if x == '1'.encode() else False for x in Dataset_airlines['Delay']]\n",
        "Dataset_airlines = Dataset_airlines.drop(['Delay'],axis=1)\n",
        "\n",
        "Dataset_airlines = Dataset_airlines.drop_duplicates()\n",
        "\n",
        "\n",
        "Dataset_airlines['Airline'] = Dataset_airlines['Airline'].astype('category')\n",
        "Dataset_airlines['AirportFrom'] = Dataset_airlines['AirportFrom'].astype('category')\n",
        "Dataset_airlines['AirportTo'] = Dataset_airlines['AirportTo'].astype('category')\n",
        "\n",
        "Dataset_airlines['Airline'] = Dataset_airlines['Airline'].cat.codes\n",
        "Dataset_airlines['AirportFrom'] = Dataset_airlines['AirportFrom'].cat.codes\n",
        "Dataset_airlines['AirportTo'] = Dataset_airlines['AirportTo'].cat.codes\n",
        "\n",
        "Dataset_airlines = Dataset_airlines.drop(removeColumns(Dataset_airlines),axis=1)\n",
        "Dataset_airlines.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "Dataset_AedesSex = InitDataset_AedesSex.copy()\n",
        "Dataset_AedesSex['Target'] = [True if x == 'F' else False for x in Dataset_AedesSex['sex']]\n",
        "Dataset_AedesSex = Dataset_AedesSex.drop(['sex'],axis=1)\n",
        "Dataset_AedesSex = Dataset_AedesSex.drop(removeColumns(Dataset_AedesSex),axis=1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3054 1586\n"
          ]
        }
      ],
      "source": [
        "Dataset_phoneme = InitDataset_phoneme.copy()\n",
        "Dataset_phoneme['Target'] = [True if x == '1'.encode() else False for x in Dataset_phoneme['Class']]\n",
        "Dataset_phoneme = Dataset_phoneme.drop(['Class'],axis=1)\n",
        "Dataset_phoneme = Dataset_phoneme.drop(removeColumns(Dataset_phoneme),axis=1)\n",
        "Dataset_phoneme = overSampling(underSampling(Dataset_phoneme))#Aplicando over e undersampling para 15% dos dados para tentar deixar mais proporcional o dataset\n",
        "Dataset_phoneme.reset_index(inplace=True, drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "Dataset_adult = InitDataset_adult.copy()\n",
        "Dataset_adult['Target'] = [True if x == '<=50K'.encode() else False for x in Dataset_adult['class']]\n",
        "Dataset_adult = Dataset_adult.drop(['class'],axis=1)\n",
        "Dataset_adult['workclass'] = [x.decode('UTF-8') for x in Dataset_adult['workclass']]\n",
        "Dataset_adult['education'] = [x.decode('UTF-8') for x in Dataset_adult['education']]\n",
        "Dataset_adult['marital-status'] = [x.decode('UTF-8') for x in Dataset_adult['marital-status']]\n",
        "Dataset_adult['occupation'] = [x.decode('UTF-8') for x in Dataset_adult['occupation']]\n",
        "Dataset_adult['relationship'] = [x.decode('UTF-8') for x in Dataset_adult['relationship']]\n",
        "Dataset_adult['race'] = [x.decode('UTF-8') for x in Dataset_adult['race']]\n",
        "Dataset_adult['sex'] = [x.decode('UTF-8') for x in Dataset_adult['sex']]\n",
        "Dataset_adult['native-country'] = [x.decode('UTF-8') for x in Dataset_adult['native-country']]\n",
        "\n",
        "Dataset_adult = Dataset_adult.dropna() #Remoção das linhas com valores faltantes\n",
        "Dataset_adult = get_dummies(Dataset_adult,'workclass')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'education')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'marital-status')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'occupation')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'relationship')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'race')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'sex')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'native-country')\n",
        "Dataset_adult = Dataset_adult.drop(removeColumns(Dataset_adult),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "##Normalizando todos os dados\n",
        "NOR_Dataset_PhishingWebsites = normalize(Dataset_PhishingWebsites)\n",
        "NOR_Dataset_arrhythmia = normalize(Dataset_arrhythmia)\n",
        "NOR_Dataset_Satellite = normalize(Dataset_Satellite)\n",
        "NOR_Dataset_AedesSex = normalize(Dataset_AedesSex)\n",
        "NOR_Dataset_airlines = normalize(Dataset_airlines)\n",
        "NOR_Dataset_phoneme = normalize(Dataset_phoneme)\n",
        "NOR_Dataset_adult = normalize(Dataset_adult)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# knn(NOR_Dataset_PhishingWebsites) #93.8498643352427\n",
        "knn(NOR_Dataset_arrhythmia) \n",
        "# knn(NOR_Dataset_Satellite) #99.2156862745098\n",
        "# knn(NOR_Dataset_AedesSex) #98.73611111111111\n",
        "# knn(NOR_Dataset_phoneme) #88.03945745992601\n",
        "# knn(NOR_Dataset_adult) #81.71022998703337\n",
        "# knn(NOR_Dataset_airlines) #81.71022998703337\n",
        "\n",
        "\n",
        "# treeDecision(NOR_Dataset_PhishingWebsites)#96.02050045221586\n",
        "# treeDecision(NOR_Dataset_arrhythmia)\n",
        "# treeDecision(NOR_Dataset_Satellite)#99.08496732026144\n",
        "# treeDecision(NOR_Dataset_AedesSex)#98.19444444444444\n",
        "# treeDecision(NOR_Dataset_phoneme)#87.05302096177559\n",
        "# treeDecision(NOR_Dataset_adult)#82.01733433426602\n",
        "# treeDecision(NOR_Dataset_airlines)\n",
        "\n",
        "\n",
        "# RForestDecision(NOR_Dataset_PhishingWebsites)#96.80434127223396\n",
        "# RForestDecision(NOR_Dataset_arrhythmia)\n",
        "# RForestDecision(NOR_Dataset_Satellite)#99.281045751634\n",
        "# RForestDecision(NOR_Dataset_AedesSex)#98.38888888888889\n",
        "# RForestDecision(NOR_Dataset_phoneme)#90.9987669543773\n",
        "# RForestDecision(NOR_Dataset_adult)#85.68893741895857\n",
        "# RForestDecision(NOR_Dataset_airlines)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "##Separando os atributos dos datasets para testar qual o melhor algoritmo\n",
        "PhishingWebsites_xTrain, PhishingWebsites_xTest, PhishingWebsites_yTrain, PhishingWebsites_yTest = train_test_split(NOR_Dataset_PhishingWebsites.drop([\"Target\"], axis=1), NOR_Dataset_PhishingWebsites[\"Target\"], test_size=0.3)\n",
        "\n",
        "Satellite_xTrain, Satellite_xTest, Satellite_yTrain, Satellite_yTest = train_test_split(NOR_Dataset_Satellite.drop([\"Target\"], axis=1), NOR_Dataset_Satellite[\"Target\"], test_size=0.3)\n",
        "\n",
        "AedesSex_xTrain, AedesSex_xTest, AedesSex_yTrain, AedesSex_yTest = train_test_split(NOR_Dataset_AedesSex.drop([\"Target\"], axis=1), NOR_Dataset_AedesSex[\"Target\"], test_size=0.3)\n",
        "\n",
        "phoneme_xTrain, phoneme_xTest, phoneme_yTrain, phoneme_yTest = train_test_split(NOR_Dataset_phoneme.drop([\"Target\"], axis=1), NOR_Dataset_phoneme[\"Target\"], test_size=0.3)\n",
        "\n",
        "adult_xTrain, adult_xTest, adult_yTrain, adult_yTest = train_test_split(NOR_Dataset_adult.drop([\"Target\"], axis=1), NOR_Dataset_adult[\"Target\"], test_size=0.3)\n",
        "\n",
        "airlines_xTrain, airlines_xTest, airlines_yTrain, airlines_yTest = train_test_split(NOR_Dataset_airlines.drop([\"Target\"], axis=1), NOR_Dataset_airlines[\"Target\"], test_size=0.3)\n",
        "\n",
        "arrhythmia_xTrain, arrhythmia_xTest, arrhythmia_yTrain, arrhythmia_yTest = train_test_split(NOR_Dataset_arrhythmia.drop([\"Target\"], axis=1), NOR_Dataset_arrhythmia[\"Target\"], test_size=0.3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/lucas/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
            "  warnings.warn(\n",
            "/home/lucas/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/home/lucas/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
            "  warnings.warn(\n",
            "/home/lucas/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1580: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n",
            "/home/lucas/.local/lib/python3.10/site-packages/sklearn/model_selection/_split.py:676: UserWarning: The least populated class in y has only 9 members, which is less than n_splits=10.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "##Calculando a media de todos os datasets a partir do cross validation juntamente com o knn\n",
        "CrossValidationDTPhishingWebsites,CrossValidationRFPhishingWebsites, CrossValidationKnnPhishingWebsites = crossVal(PhishingWebsites_xTest,PhishingWebsites_yTest,PhishingWebsites_xTrain,PhishingWebsites_yTrain) \n",
        "CrossValidationDTSatellite,CrossValidationRFSatellite, CrossValidationKnnSatellite = crossVal(Satellite_xTest,Satellite_yTest,Satellite_xTrain,Satellite_yTrain) \n",
        "CrossValidationDTAedesSex,CrossValidationRFAedesSex, CrossValidationKnnAedesSex = crossVal(AedesSex_xTest,AedesSex_yTest,AedesSex_xTrain,AedesSex_yTrain) \n",
        "CrossValidationDTPhoneme,CrossValidationRFPhoneme, CrossValidationKnnPhoneme = crossVal(phoneme_xTest,phoneme_yTest,phoneme_xTrain,phoneme_yTrain) \n",
        "CrossValidationDTAdult,CrossValidationRFAdult, CrossValidationKnnAdult = crossVal(adult_xTest,adult_yTest,adult_xTrain,adult_yTrain) \n",
        "CrossValidationDTArrhythmia,CrossValidationRFArrhythmia, CrossValidationKnnArrhythmia = crossVal(arrhythmia_xTest,arrhythmia_yTest,arrhythmia_xTrain,arrhythmia_yTrain) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "26.666666666666668 36.666666666666664 36.666666666666664\n"
          ]
        }
      ],
      "source": [
        "# print(CrossValidationDTPhishingWebsites.mean()*100,CrossValidationRFPhishingWebsites.mean()*100, CrossValidationKnnPhishingWebsites.mean()*100)\n",
        "# print(CrossValidationDTSatellite.mean()*100,CrossValidationRFSatellite.mean()*100, CrossValidationKnnSatellite.mean()*100)\n",
        "# print(CrossValidationDTAedesSex.mean()*100,CrossValidationRFAedesSex.mean()*100, CrossValidationKnnAedesSex.mean()*100)\n",
        "# print(CrossValidationDTPhoneme.mean()*100,CrossValidationRFPhoneme.mean()*100, CrossValidationKnnPhoneme.mean()*100)\n",
        "# print(CrossValidationDTAdult.mean()*100,CrossValidationRFAdult.mean()*100, CrossValidationKnnAdult.mean()*100)\n",
        "print(CrossValidationDTArrhythmia.mean()*100,CrossValidationRFArrhythmia.mean()*100, CrossValidationKnnArrhythmia.mean()*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "CrossValidationDTAirlines,CrossValidationRFAirlines, CrossValidationKnnAirlines = crossVal(airlines_xTest,airlines_yTest,airlines_xTrain,airlines_yTrain) \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fazendo a analise de desempenho separadamente dos algoritmos utilizando f1-Score:\n",
        "\n",
        "* Fórmula f1-Score\n",
        "\n",
        "F1 = 2 * (precision * recall) / (precision + recall)\n",
        "* PhishingWebsites: O melhor foi o random florest que teve um score melhor que os outros, ficando quase 1% melhor que o knn e 0.1% melhor que o Decision Tree. \n",
        "* Satellite: O melhor foi o random florest que teve um score melhor que os outros, ficando quase 21% melhor que o knn e 0.13% melhor que o Decision Tree. \n",
        "* AedesSex: O melhor foi o random florest que teve um score melhor que os outros, ficando quase 3% melhor que o knn e 0.4% melhor que o Decision Tree. \n",
        "* Phoneme: O melhor foi o random florest que teve um score melhor que os outros, ficando quase 3% melhor que o knn e 3% melhor que o Decision Tree. \n",
        "* Adult: O melhor foi o KNN que teve um score melhor que os outros, ficando quase 0.2% melhor que o random florest e 2% melhor que o Decision Tree. \n",
        "* Arrhythmia: Para este algoritmo todos ficaram extremamente ruins, e o knn e random florest ficaram com o mesmo score de 36.6.\n",
        "* Airlines: No caso do airlines os scores também ficaram ruins,, no entanto o knn foi melhor que os outros, ficando com 47.98 de score, enquanto o Random florest ficou com 45.85 e o Decision tree 42.27.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "42.270785459424935 45.85076131956758 47.98148987475436\n"
          ]
        }
      ],
      "source": [
        "print(CrossValidationDTAirlines.mean()*100,CrossValidationRFAirlines.mean()*100, CrossValidationKnnAirlines.mean()*100)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {},
      "outputs": [],
      "source": [
        "NArrayDT = np.array([])\n",
        "NArrayDT = np.append(NArrayDT, CrossValidationDTPhishingWebsites)\n",
        "NArrayDT = np.append(NArrayDT, CrossValidationDTSatellite)\n",
        "NArrayDT = np.append(NArrayDT, CrossValidationDTAedesSex)\n",
        "NArrayDT = np.append(NArrayDT, CrossValidationDTPhoneme)\n",
        "NArrayDT = np.append(NArrayDT, CrossValidationDTAdult)\n",
        "NArrayDT = np.append(NArrayDT, CrossValidationDTArrhythmia)\n",
        "NArrayDT = np.append(NArrayDT, CrossValidationDTAirlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "NArrayRF = np.array([])\n",
        "NArrayRF = np.append(NArrayRF,CrossValidationRFPhishingWebsites)\n",
        "NArrayRF = np.append(NArrayRF,CrossValidationRFSatellite)\n",
        "NArrayRF = np.append(NArrayRF,CrossValidationRFAedesSex)\n",
        "NArrayRF = np.append(NArrayRF,CrossValidationRFPhoneme)\n",
        "NArrayRF = np.append(NArrayRF,CrossValidationRFAdult)\n",
        "NArrayRF = np.append(NArrayRF,CrossValidationRFArrhythmia)\n",
        "NArrayRF = np.append(NArrayRF,CrossValidationRFAirlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [],
      "source": [
        "NArrayKnn = np.array([])\n",
        "NArrayKnn = np.append(NArrayKnn,CrossValidationKnnPhishingWebsites)\n",
        "NArrayKnn = np.append(NArrayKnn,CrossValidationKnnSatellite)\n",
        "NArrayKnn = np.append(NArrayKnn,CrossValidationKnnAedesSex)\n",
        "NArrayKnn = np.append(NArrayKnn,CrossValidationKnnPhoneme)\n",
        "NArrayKnn = np.append(NArrayKnn,CrossValidationKnnAdult)\n",
        "NArrayKnn = np.append(NArrayKnn,CrossValidationKnnArrhythmia)\n",
        "NArrayKnn = np.append(NArrayKnn,CrossValidationKnnAirlines)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "70.55488456804274 75.22450969655753 71.86325394350933\n"
          ]
        }
      ],
      "source": [
        "print(NArrayDT.mean()*100,NArrayRF.mean()*100,NArrayKnn.mean()*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Como algoritmo mais genérico, o que teve o melhor score foi o Random florest com 75.22% e em segundo o knn."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Trabalho_1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
