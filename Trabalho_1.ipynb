{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHO6AESuxnGM"
      },
      "source": [
        "# Trabalho 1 - Nivelamento\n",
        "\n",
        "Considere os datasets abaixo, estratégias de pré-processamento, medidas de avaliação, métodos de comparação estatística e os seguintes algoritmos de aprendizado de máquina: árvore de decisão, random forest e k-nearest neighbor. A partir disso, responda as seguintes perguntas:\n",
        "\n",
        "1. Qual o algoritmo de AM mais adequado para cada problema?\n",
        "2. Qual o algoritmo de AM mais adequado para todos os problemas?\n",
        "\n",
        "Para responder essas questões construa um notebook no colab ou um ambiente similar. Documente de forma clara cada passo e justifique suas decisões."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LOdk7BJHxmH4"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import confusion_matrix,accuracy_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.feature_selection import RFE\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib as plt\n",
        "from scipy.io import arff\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import io"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "PhishingWebsites_link = \"https://www.openml.org/data/download/1798106/phpV5QYya\"\n",
        "arrhythmia_link       = \"https://www.openml.org/data/download/53551/arrhythmia.arff\"\n",
        "Satellite_link        = \"https://www.openml.org/data/download/16787463/phpZrCzJR\"\n",
        "airlines_link         = \"https://www.openml.org/data/download/66526/phpvcoG8S\"\n",
        "AedesSex_link         = \"https://github.com/denismr/Classification-and-Counting-with-Recurrent-Contexts/raw/master/codeAndData/data/AedesSex.csv\"\n",
        "phoneme_link          = \"https://www.openml.org/data/download/1592281/php8Mz7BG\"\n",
        "adult_link            = \"https://www.openml.org/data/download/1595261/phpMawTba\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "def loadDataset_Arff(url: str) -> pd.DataFrame:\n",
        "  ftpstream = urllib.request.urlopen(url)\n",
        "  return pd.DataFrame(arff.loadarff(io.StringIO(ftpstream.read().decode('utf-8')))[0])\n",
        "  \n",
        "def get_dummies(df: pd.DataFrame, col:str) -> pd.DataFrame:\n",
        "  return pd.get_dummies(df,prefix=col,prefix_sep='.',columns=[col]).copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def select_nominals(dataset: pd.DataFrame) -> list:\n",
        "    \"\"\"\n",
        "    Seleciona os atributos não numéricos do dataset.\n",
        "    \"\"\"\n",
        "    att: list = []\n",
        "    for i, v in dataset.dtypes.items():\n",
        "        if(v == \"object\"):\n",
        "            att.append(i)\n",
        "    return att\n",
        "\n",
        "def is_binary_nominal(nominals: list) -> \"tuple[bool, list[str]]\":\n",
        "    \"\"\"\n",
        "    Retorna se uma lista de atributos nominais contém apenas 2 valores distintos ou não.\n",
        "    \"\"\"\n",
        "    unique_att: \"list[str]\" = []\n",
        "    for n in nominals:\n",
        "        if(not n in unique_att):\n",
        "            unique_att.append(n)\n",
        "    return len(unique_att) == 2, unique_att"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_binary_nominal_to_numeric(dataset: pd.DataFrame, attribute: str) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converte atributos nominais binários de um dataset para numéricos binários (0 ou 1).\n",
        "    \"\"\"\n",
        "    new_dt = dataset.copy()\n",
        "    attributes = new_dt[attribute]\n",
        "    unq_attr: dict['str', int] = {}\n",
        "    # Pegando os atributos únicos\n",
        "    i = 0\n",
        "    for a in attributes:\n",
        "        if(not a in unq_attr):\n",
        "            unq_attr[a] = i\n",
        "            i += 1\n",
        "    # Substituindo os valores nominais por numéricos.\n",
        "    for u in unq_attr:\n",
        "        new_dt.loc[new_dt[attribute] == u, attribute] = unq_attr[u]\n",
        "    return new_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_nominal_to_numeric(dataset: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Converte todos os atributos nominais de um dataset para atributos numéricos.\n",
        "    \"\"\"\n",
        "    nominals = select_nominals(dataset)\n",
        "    new_dt = dataset.copy()\n",
        "    for att in nominals:\n",
        "        binary, values = is_binary_nominal(new_dt[att])\n",
        "        if(binary): # Atributo nominal binário\n",
        "            new_dt = convert_binary_nominal_to_numeric(new_dt, att)\n",
        "        else: # Atributo nominal não binário\n",
        "            new_dt = get_dummies(new_dt, att)\n",
        "    return new_dt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "def proportion(Dataset,target=\"Target\"):\n",
        "  target_count = Dataset[target].value_counts()\n",
        "  print('Class 0:', target_count[0])\n",
        "  print('Class 1:', target_count[1])\n",
        "  target_count.plot(kind='bar', title='Count (target)');\n",
        "  \n",
        "def normalize(dataset,target='Target') -> pd.DataFrame:\n",
        "  # Separação dos dados preditivos e dos valores\n",
        "  X = dataset.drop([target], axis=1).copy()\n",
        "  Y = dataset[target].copy()\n",
        "\n",
        "  # Normalização\n",
        "  scaler = MinMaxScaler()\n",
        "  X = pd.DataFrame(scaler.fit_transform(X), columns=X.columns)\n",
        "  dataset = X\n",
        "  dataset[target] = Y\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "def knn(dataset,n_neighbors=3,target='Target'):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(dataset.drop([target], axis=1), dataset[target], test_size=0.3)\n",
        "  model = KNeighborsClassifier(n_neighbors=3)\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  print(accuracy_score(y_test, y_pred)*100)\n",
        "\n",
        "def treeDecision(dataset,target='Target'):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(dataset.drop([target], axis=1), dataset[target], test_size=0.3)\n",
        "  model = DecisionTreeClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  print(accuracy_score(y_test, y_pred)*100)\n",
        "\n",
        "def RForestDecision(dataset,target='Target'):\n",
        "  X_train, X_test, y_train, y_test = train_test_split(dataset.drop([target], axis=1), dataset[target], test_size=0.3)\n",
        "  model = RandomForestClassifier()\n",
        "  model.fit(X_train, y_train)\n",
        "  y_pred = model.predict(X_test)\n",
        "  print(accuracy_score(y_test, y_pred)*100)\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# y = df_balanced['Target']\n",
        "# x = df_balanced.drop(['Target'],axis=1)\n",
        "# classificador = KNeighborsClassifier(n_neighbors=5,metric='euclidean')\n",
        "# cv_result_enhanced = cross_val_score(classificador, x, y, cv=20, scoring=\"accuracy\")\n",
        "# print(\"Acurácia com cross validation:\", cv_result_enhanced.mean()*100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [],
      "source": [
        "def TTS_knn(dataset, x ,y ,n_neighbors=3):\n",
        "  model = KNeighborsClassifier(n_neighbors=3)\n",
        "  return cross_val_score(model, x, y, cv=20, scoring=\"accuracy\")\n",
        "\n",
        "def TTS_treeDecision(dataset, x ,y):\n",
        "  model = DecisionTreeClassifier()\n",
        "  return cross_val_score(model, x, y, cv=20, scoring=\"accuracy\")\n",
        "\n",
        "def TTS_RForestDecision(dataset, x ,y):\n",
        "  model = RandomForestClassifier()\n",
        "  return cross_val_score(model, x, y, cv=20, scoring=\"accuracy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "def selectAttributes(dataset):\n",
        "  model = DecisionTreeClassifier()#max_leaf_nodes=10\n",
        "  feature_ = RFE(model,n_features_to_select=5, step=1)\n",
        "  return feature_.fit(dataset.drop(['Target'],axis=1),dataset['Target']).support_\n",
        "\n",
        "def removeColumns(dataset):\n",
        "  attributes = selectAttributes(dataset)\n",
        "  dropListIndex = np.where(attributes == False)[0]\n",
        "  dropList = []\n",
        "  for index in dropListIndex:\n",
        "    dropList.append(dataset.columns[index])\n",
        "  return dropList\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Lendo todos os datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "InitDataset_PhishingWebsites = loadDataset_Arff(PhishingWebsites_link)\n",
        "InitDataset_arrhythmia = loadDataset_Arff(arrhythmia_link)\n",
        "InitDataset_Satellite = loadDataset_Arff(Satellite_link)\n",
        "InitDataset_airlines = loadDataset_Arff(airlines_link)\n",
        "InitDataset_AedesSex = pd.read_csv(AedesSex_link)\n",
        "InitDataset_phoneme = loadDataset_Arff(phoneme_link)\n",
        "InitDataset_adult = loadDataset_Arff(adult_link)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Abaixo será feito a conversão das classes targets para True e False, e trocado o nome da coluna para Target para as que não estão. Também será feito uma analise de distribuição e procura de dados nominais."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Dataset_PhishingWebsites = InitDataset_PhishingWebsites.copy()\n",
        "Dataset_PhishingWebsites['Target'] = [True if x == '-1'.encode() else False for x in Dataset_PhishingWebsites['Result']]\n",
        "Dataset_PhishingWebsites = Dataset_PhishingWebsites.drop(['Result'],axis=1)\n",
        "Dataset_PhishingWebsites = Dataset_PhishingWebsites.drop(removeColumns(Dataset_PhishingWebsites),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Dataset_arrhythmia = InitDataset_arrhythmia.copy()\n",
        "Dataset_arrhythmia['Target'] = [True if x == 'P'.encode() else False for x in Dataset_arrhythmia['binaryClass']]\n",
        "Dataset_arrhythmia = Dataset_arrhythmia.drop(['binaryClass'],axis=1) #Remoção da coluna J por ter 98% dos dados NaN\n",
        "Dataset_arrhythmia = Dataset_arrhythmia.drop(removeColumns(Dataset_arrhythmia),axis=1) \n",
        "Dataset_arrhythmia = Dataset_arrhythmia.dropna() #Remoção das linhas com valores faltantes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##Sem nominais usando a função select_nominals\n",
        "Dataset_Satellite = InitDataset_Satellite.copy()\n",
        "Dataset_Satellite['Target'] = [True if x == 'Anomaly'.encode() else False for x in Dataset_Satellite['Target']]\n",
        "Dataset_Satellite = Dataset_Satellite.drop(removeColumns(Dataset_Satellite),axis=1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##Dataset com muitos dados nominais, para aplicar o knn será necessário usar a função getdummies em AirportFrom e AirportTo.\n",
        "Dataset_airlines = InitDataset_airlines.copy()\n",
        "Dataset_airlines['Target'] = [True if x == '1'.encode() else False for x in Dataset_airlines['Delay']]\n",
        "Dataset_airlines = Dataset_airlines.drop(['Delay'],axis=1)\n",
        "\n",
        "Dataset_airlines['Airline'] = Dataset_airlines['Airline'].astype('category')\n",
        "Dataset_airlines['AirportFrom'] = Dataset_airlines['AirportFrom'].astype('category')\n",
        "Dataset_airlines['AirportTo'] = Dataset_airlines['AirportTo'].astype('category')\n",
        "\n",
        "Dataset_airlines['Airline'] = Dataset_airlines['Airline'].cat.codes\n",
        "Dataset_airlines['AirportFrom'] = Dataset_airlines['AirportFrom'].cat.codes\n",
        "Dataset_airlines['AirportTo'] = Dataset_airlines['AirportTo'].cat.codes\n",
        "\n",
        "Dataset_airlines = Dataset_airlines.drop(removeColumns(Dataset_airlines),axis=1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Dataset_AedesSex = InitDataset_AedesSex.copy()\n",
        "Dataset_AedesSex['Target'] = [True if x == 'F' else False for x in Dataset_AedesSex['sex']]\n",
        "Dataset_AedesSex = Dataset_AedesSex.drop(['sex'],axis=1)\n",
        "Dataset_AedesSex = Dataset_AedesSex.drop(removeColumns(Dataset_AedesSex),axis=1) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "##Dataset desbalanceado, analisar como se comporta nos algoritmos de aprendizagem para saber se é necessário balancear.\n",
        "Dataset_phoneme = InitDataset_phoneme.copy()\n",
        "Dataset_phoneme['Target'] = [True if x == '1'.encode() else False for x in Dataset_phoneme['Class']]\n",
        "Dataset_phoneme = Dataset_phoneme.drop(['Class'],axis=1)\n",
        "Dataset_phoneme = Dataset_phoneme.drop(removeColumns(Dataset_phoneme),axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ##Dataset com muitas colunas nominais, será necessário gerar o dummie das mesmas.\n",
        "Dataset_adult = InitDataset_adult.copy()\n",
        "Dataset_adult['Target'] = [True if x == '<=50K'.encode() else False for x in Dataset_adult['class']]\n",
        "Dataset_adult = Dataset_adult.drop(['class'],axis=1)\n",
        "Dataset_adult['workclass'] = [x.decode('UTF-8') for x in Dataset_adult['workclass']]\n",
        "Dataset_adult['education'] = [x.decode('UTF-8') for x in Dataset_adult['education']]\n",
        "Dataset_adult['marital-status'] = [x.decode('UTF-8') for x in Dataset_adult['marital-status']]\n",
        "Dataset_adult['occupation'] = [x.decode('UTF-8') for x in Dataset_adult['occupation']]\n",
        "Dataset_adult['relationship'] = [x.decode('UTF-8') for x in Dataset_adult['relationship']]\n",
        "Dataset_adult['race'] = [x.decode('UTF-8') for x in Dataset_adult['race']]\n",
        "Dataset_adult['sex'] = [x.decode('UTF-8') for x in Dataset_adult['sex']]\n",
        "Dataset_adult['native-country'] = [x.decode('UTF-8') for x in Dataset_adult['native-country']]\n",
        "\n",
        "Dataset_adult = Dataset_adult.dropna() #Remoção das linhas com valores faltantes\n",
        "\n",
        "Dataset_adult = get_dummies(Dataset_adult,'workclass')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'education')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'marital-status')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'occupation')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'relationship')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'race')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'sex')\n",
        "Dataset_adult = get_dummies(Dataset_adult,'native-country')\n",
        "Dataset_adult = Dataset_adult.drop(removeColumns(Dataset_adult),axis=1)\n",
        "# Dataset_adult"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Normalização de todos os datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NOR_Dataset_PhishingWebsites = normalize(Dataset_PhishingWebsites)\n",
        "NOR_Dataset_arrhythmia = normalize(Dataset_arrhythmia)\n",
        "NOR_Dataset_Satellite = normalize(Dataset_Satellite) #DESBALANCEADO\n",
        "NOR_Dataset_AedesSex = normalize(Dataset_AedesSex)\n",
        "NOR_Dataset_phoneme = normalize(Dataset_phoneme)\n",
        "NOR_Dataset_adult = normalize(Dataset_adult)\n",
        "NOR_Dataset_airlines = normalize(Dataset_airlines) # Dataset estourando a memoria.\n",
        "\n",
        "# NOR_Dataset_arrhythmia = NOR_Dataset_arrhythmia.dropna()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Aplicando o knn em todos os datasets para saber a acurácia."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "60.834286067422674\n",
            "60.81945431511294\n",
            "61.78290022556623\n"
          ]
        }
      ],
      "source": [
        "knn(NOR_Dataset_PhishingWebsites) #93.8498643352427\n",
        "# knn(NOR_Dataset_arrhythmia) \n",
        "knn(NOR_Dataset_Satellite) #99.2156862745098\n",
        "knn(NOR_Dataset_AedesSex) #98.73611111111111\n",
        "knn(NOR_Dataset_phoneme) #88.03945745992601\n",
        "knn(NOR_Dataset_adult) #81.71022998703337\n",
        "knn(NOR_Dataset_airlines) #81.71022998703337\n",
        "\n",
        "\n",
        "treeDecision(NOR_Dataset_PhishingWebsites)#96.02050045221586\n",
        "# treeDecision(NOR_Dataset_arrhythmia)\n",
        "treeDecision(NOR_Dataset_Satellite)#99.08496732026144\n",
        "treeDecision(NOR_Dataset_AedesSex)#98.19444444444444\n",
        "treeDecision(NOR_Dataset_phoneme)#87.05302096177559\n",
        "treeDecision(NOR_Dataset_adult)#82.01733433426602\n",
        "treeDecision(NOR_Dataset_airlines)\n",
        "\n",
        "\n",
        "RForestDecision(NOR_Dataset_PhishingWebsites)#96.80434127223396\n",
        "# RForestDecision(NOR_Dataset_arrhythmia)\n",
        "RForestDecision(NOR_Dataset_Satellite)#99.281045751634\n",
        "RForestDecision(NOR_Dataset_AedesSex)#98.38888888888889\n",
        "RForestDecision(NOR_Dataset_phoneme)#90.9987669543773\n",
        "RForestDecision(NOR_Dataset_adult)#85.68893741895857\n",
        "RForestDecision(NOR_Dataset_airlines)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "NOR_Dataset_PhishingWebsites_y = NOR_Dataset_PhishingWebsites['Target']\n",
        "NOR_Dataset_PhishingWebsites_x = NOR_Dataset_PhishingWebsites.drop(['Target'],axis=1)\n",
        "\n",
        "NOR_Dataset_Satellite_y = NOR_Dataset_Satellite['Target']\n",
        "NOR_Dataset_Satellite_x = NOR_Dataset_Satellite.drop(['Target'],axis=1)\n",
        "\n",
        "NOR_Dataset_AedesSex_y = NOR_Dataset_AedesSex['Target']\n",
        "NOR_Dataset_AedesSex_x = NOR_Dataset_AedesSex.drop(['Target'],axis=1)\n",
        "\n",
        "NOR_Dataset_phoneme_y = NOR_Dataset_phoneme['Target']\n",
        "NOR_Dataset_phoneme_x = NOR_Dataset_phoneme.drop(['Target'],axis=1)\n",
        "\n",
        "NOR_Dataset_adult_y = NOR_Dataset_adult['Target']\n",
        "NOR_Dataset_adult_x = NOR_Dataset_adult.drop(['Target'],axis=1)\n",
        "\n",
        "NOR_Dataset_airlines_y = NOR_Dataset_airlines['Target']\n",
        "NOR_Dataset_airlines_x = NOR_Dataset_airlines.drop(['Target'],axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "92.73600233667308"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "precisionListKnn = np.array([])\n",
        "precisionListKnn = np.append(precisionListKnn,TTS_knn(NOR_Dataset_PhishingWebsites,NOR_Dataset_PhishingWebsites_x,NOR_Dataset_PhishingWebsites_y) ) \n",
        "precisionListKnn = np.append(precisionListKnn,TTS_knn(NOR_Dataset_Satellite,NOR_Dataset_Satellite_x,NOR_Dataset_Satellite_y) ) \n",
        "precisionListKnn = np.append(precisionListKnn,TTS_knn(NOR_Dataset_AedesSex,NOR_Dataset_AedesSex_x,NOR_Dataset_AedesSex_y) ) \n",
        "precisionListKnn = np.append(precisionListKnn,TTS_knn(NOR_Dataset_phoneme,NOR_Dataset_phoneme_x,NOR_Dataset_phoneme_y) ) \n",
        "precisionListKnn = np.append(precisionListKnn,TTS_knn(NOR_Dataset_adult,NOR_Dataset_adult_x,NOR_Dataset_adult_y) ) \n",
        "precisionListKnn = np.append(precisionListKnn,TTS_knn(NOR_Dataset_airlines,NOR_Dataset_airlines_x,NOR_Dataset_airlines_y) ) \n",
        "precisionListKnn.mean()*100\n",
        "\n",
        "\n",
        "precisionListTree = np.array([])\n",
        "precisionListTree = np.append(precisionListTree,TTS_treeDecision(NOR_Dataset_PhishingWebsites,NOR_Dataset_PhishingWebsites_x,NOR_Dataset_PhishingWebsites_y) ) \n",
        "precisionListTree = np.append(precisionListTree,TTS_treeDecision(NOR_Dataset_Satellite,NOR_Dataset_Satellite_x,NOR_Dataset_Satellite_y) ) \n",
        "precisionListTree = np.append(precisionListTree,TTS_treeDecision(NOR_Dataset_AedesSex,NOR_Dataset_AedesSex_x,NOR_Dataset_AedesSex_y) ) \n",
        "precisionListTree = np.append(precisionListTree,TTS_treeDecision(NOR_Dataset_phoneme,NOR_Dataset_phoneme_x,NOR_Dataset_phoneme_y) ) \n",
        "precisionListTree = np.append(precisionListTree,TTS_treeDecision(NOR_Dataset_adult,NOR_Dataset_adult_x,NOR_Dataset_adult_y) ) \n",
        "precisionListTree = np.append(precisionListTree,TTS_treeDecision(NOR_Dataset_airlines,NOR_Dataset_airlines_x,NOR_Dataset_airlines_y) ) \n",
        "precisionListTree.mean()*100\n",
        "\n",
        "precisionListRf = np.array([])\n",
        "precisionListRf = np.append(precisionListRf,TTS_RForestDecision(NOR_Dataset_PhishingWebsites,NOR_Dataset_PhishingWebsites_x,NOR_Dataset_PhishingWebsites_y) ) \n",
        "precisionListRf = np.append(precisionListRf,TTS_RForestDecision(NOR_Dataset_Satellite,NOR_Dataset_Satellite_x,NOR_Dataset_Satellite_y) ) \n",
        "precisionListRf = np.append(precisionListRf,TTS_RForestDecision(NOR_Dataset_AedesSex,NOR_Dataset_AedesSex_x,NOR_Dataset_AedesSex_y) ) \n",
        "precisionListRf = np.append(precisionListRf,TTS_RForestDecision(NOR_Dataset_phoneme,NOR_Dataset_phoneme_x,NOR_Dataset_phoneme_y) ) \n",
        "precisionListRf = np.append(precisionListRf,TTS_RForestDecision(NOR_Dataset_adult,NOR_Dataset_adult_x,NOR_Dataset_adult_y) ) \n",
        "precisionListRf = np.append(precisionListRf,TTS_RForestDecision(NOR_Dataset_airlines,NOR_Dataset_airlines_x,NOR_Dataset_airlines_y) ) \n",
        "precisionListRf.mean()*100\n",
        "\n",
        "print((precisionListKnn.mean()*100),(precisionListTree.mean()*100),(precisionListRf.mean()*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Trabalho_1.ipynb",
      "provenance": []
    },
    "interpreter": {
      "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
    },
    "kernelspec": {
      "display_name": "Python 3.10.2 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
